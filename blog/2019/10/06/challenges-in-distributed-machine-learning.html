<!DOCTYPE html>
<html lang="en">

  <head>

    <!-- Non social metatags -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="theme-color" content="#157878">

    

    <title>Challenges In Distributed Machine Learning</title>

    
      <!-- Update your html tag to include the itemscope and itemtype attributes. -->
<html itemscope itemtype="http://schema.org/Article">












<!-- Place this data between the <head> tags of your website -->

  <meta name="author" content="Christian Cosgrove">

<meta name="description" content="" />





<!-- Schema.org markup for Google+ -->
<meta itemprop="name" content="Challenges In Distributed Machine Learning">
<meta itemprop="description" content="">

  <meta itemprop="image" content="http://localhost:4000/blog">


<!-- Twitter Card data -->
<meta name="twitter:card" content="summary_large_image">



<meta name="twitter:title" content="Challenges In Distributed Machine Learning">
<meta name="twitter:description" content="">



<!-- Twitter summary card with large image must be at least 280x150px -->

  <meta name="twitter:image:src" content="http://localhost:4000/blog">
  <meta property="twitter:image" content="http://localhost:4000/blog">

<meta property="twitter:url" content="http://localhost:4000/blog/2019/10/06/challenges-in-distributed-machine-learning.html">

<!-- Open Graph data -->
<meta property="og:title" content="Challenges In Distributed Machine Learning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:4000/blog/2019/10/06/challenges-in-distributed-machine-learning.html" />

  <meta property="og:image" content="http://localhost:4000/blog" />

<meta property="og:description" content="" />
<meta property="og:site_name" content="Ode to Gradients" />

  <meta property="article:published_time" content="2019-10-06T00:00:00-04:00" />














  





  




    

    <link rel="canonical" href="http://localhost:4000/blog/2019/10/06/challenges-in-distributed-machine-learning.html">

    

    <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
    <meta name="robots" content="noarchive">

    <!-- <link rel="alternate" media="only screen and (max-width: 640px)" href="">
    <link rel="alternate" media="handheld" href=""> -->


    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/blog/assets/css/style.css?v=">
  </head>
  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    

    
      <a class="site-title" href="/blog/">Ode to Gradients</a>
    

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="https://christiancosgrove.com/">About</a>
            <!-- 
            
            
              
              
            
          
            
            
          
            
            
           -->
        </div>
      </nav>
    
  </div>
</header>


    
    
    

    <section class="page-header">
      <h1 class="project-name">Challenges In Distributed Machine Learning</h1>
      <h2 class="project-tagline"></h2>
      
      <!-- Post tagline -->
      
        <h2 class="project-date">
        <time datetime="2019-10-06T00:00:00-04:00" itemprop="datePublished">
          
          Oct 6, 2019
        </time>
        
        
          • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Christian Cosgrove</span></span>
        
        </h2>
      
      <!-- End: Post tagline -->
    </section>

    <section class="main-content">

      <article itemscope itemtype="http://schema.org/BlogPosting">

  <!-- <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Challenges In Distributed Machine Learning</h1>
    <p class="post-meta">
      <time datetime="2019-10-06T00:00:00-04:00" itemprop="datePublished">
        
        Oct 6, 2019
      </time>
      </p>
  </header> -->
  <script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/0.99.6/jsxgraphcore.js"></script>
  <div itemprop="articleBody">
    <!-- 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
 -->

<p>Recently, we’ve seen the power of <em>large-scale</em> machine learning projects. Projects like <a href="https://openai.com/blog/better-language-models/">GPT-2</a> and <a href="https://arxiv.org/abs/1809.11096">BigGAN</a> involve model and dataset sizes that would be unfathomable only a few years ago, and they they are increasingly dominating the state of the art. Underlying the push for larger models and larger, more complex, and more diverse datasets is the common belief that <strong>all we need to do is scale up</strong>. But a couple different (orthogonal) approaches seem to be emerging to achieve this scale:</p>

<ol>
  <li>Better hardware. GPUs weren’t designed for deep learning, so why stop here? FPGAs, TPUs, neuromorphic computing all fall under this category.</li>
  <li>More efficient algorithms and architectures. The earliest deep learning architectures, as it turns out, used many more parameters and computation than was necessary. <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> comes to mind.</li>
  <li><strong>Distributed systems</strong>. This blog post will focus exclusively on this approach to ML scale.</li>
</ol>

<h2 id="why-distributed-ml">Why distributed ML?</h2>
<p>OpenAI has observed that since 2012 <a href="https://openai.com/blog/ai-and-compute/">AI model size is growing faster than Moore’s law</a>, with the number of flops used for training <strong>doubling every 3.5 months</strong>. While they acknowledge that hardware has improved, they attribute the majority of this increase in size to <strong>distributed training</strong>.</p>

<p>But although we haven’t hit a wall in hardware yet, the same fundamental, physical limitations will present themselves, and so I would argue that multi-machine training will become increasingly important as we try to scale up models.</p>

<p><em>Sidenote:</em> It’s important to make the distinction between distributed training and distributed inference. The case for distributed inference is somewhat obvious: if your ML service gets a lot of requests, you want to handle that load, so you distribute your model onto many machines. But scaling training is what leads to improved model accuracy, and it is a more challenging problem.</p>

<h2 id="one-way-to-distribute-training">One way to distribute training</h2>

<p>Let’s assume we have a loss function of the form (empirical risk minimization):</p>

<p>\begin{equation}
    L(\theta) = \frac{1}{N} \sum_{i=1}^N f(x_i; \theta)
\end{equation}</p>

<p>Now notice that we can partition the dataset into \(M\) parts and average over those parts
\begin{equation}
    L(\theta) = \frac{1}{M} \sum_{i\in [M]} \frac{1}{|B_i|} \sum_{i \in B_i} f(x_i; \theta)
\end{equation}
This shows that we can split up our dataset across many machines and compute the loss of each partition of the dataset independently. If we want to compute the loss, we just average the per-machine losses!</p>

<p>Because of linearity, this also holds for the <em>gradients</em>:</p>

<p>\begin{equation}
    \nabla L(\theta) = \frac{1}{M} \sum_{i\in [M]} \frac{1}{|B_i|} \sum_{i \in B_i} \nabla f(x_i; \theta)
\end{equation}
This shows how we can use <strong>data parallelism</strong> to train a model with large batch sizes over multiple machines. On each iteration, each machine samples a random subset of their partition of the data, computes a gradient estimate \(g_{B_i}(\theta)\) using backpropagation on that subset, and sends its gradient to a centralized parameter server that averages the received gradients and updates the parameters. The parameter server then sends its parameters back to the workers so that they can update their local copy of the model.</p>

<p><img src="http://localhost:4000/blog/assets/images/data_parallelism.svg" alt="An illustration of data parallelism" /></p>

<p>This gives an idea of how we can train models with massive batch sizes (since the effective batch size is the sum of the batch sizes for each machine). Larger batch sizes reduce gradient norms, which in in turn improve the convergence rate of SGD. (<a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf">Large batch sizes have been used to train models on ImageNet in less than one hour.</a>)</p>

<h2 id="distributed-ml-without-trust">Distributed ML without trust</h2>
<p>Bitcoin/cryptocurrency mining consumes a <a href="https://digiconomist.net/bitcoin-energy-consumption">substantial percentage</a> of the world’s electricity. It has also increased the price of GPUs at a time when they are increasingly valuable for deep learning.</p>

<p><strong>The question arises</strong>: can we harness the funamentally distributed and <strong>decentralized</strong> nature of computation in cryptocurrencies to train ML models? Bitcoin has demonstrated that people and organizations are willing to assemble a large amount of hardware and use electricity to make a small amount of money in return. Maybe we could do the same thing with machine learning?</p>

<p>Here’s a toy model for this type of approach:</p>
<ol>
  <li>The requester submits its “problem” to a decentralized network of nodes. This “problem” consists of a link to a dataset that can be downloaded in partitions.</li>
  <li>The nodes in this network compute gradients on their respective subsets of the data, and send this back to the requester.</li>
  <li>The requester can either trust these gradients, or compute the change in the loss itself. (in this setting, doing the latter is infeasible, so this might require new improvements in cheap gradient verification)</li>
  <li>The request pays the nodes a small amount for their gradients. This gives the nodes an incentive to continue computing gradients for the requester.</li>
</ol>

<p><img src="http://localhost:4000/blog/assets/images/decentralized_ml.svg" alt="A toy model for decentralized ML" /></p>

<p>Here are some problems with this approach</p>
<ol>
  <li><strong>Network overhead</strong> Network latency poses a problem for this approach. If a single SGD update requires us to receive the new gradients, update our local copy of the parameters, and broadcast our copy of the parameters out to the workers, we may have to wait a significant amount of time until we can compute the next update.</li>
  <li><strong>Ensuring consistency</strong>. Like all distributed systems, we need to ensure that all participants have the same copy of the parameters. I.e., how do we know that we are getting fresh gradients from a node? Ensuring consistency in an adversarial environment is more challenging.</li>
  <li><strong>Trust</strong>. How do we know that the gradients that we receive from each of the workers is <em>correct</em>, i.e., that it is an unbiased estimator of the true gradient for that partition? How do the workers know that they will receive payment for computing the gradients?</li>
</ol>

<p>The final of these is a difficult challenge, but there has been some promising work in the area. In particular, <strong>verifiable computation</strong> is the notion that we can provide a proof (certificate) with the output of our computation and use the proof to check that the output is correct <strong>without doing the computation ourselves</strong>. Verifiable computation has been extensively explored in cryptocurrencies, but there has already been some preliminary work to <a href="https://eprint.iacr.org/2017/1038.pdf">verify the output of a neural network</a>. If we can verify the output of the model, there is promise that we can use similar techniques to verify</p>

<p>Another way to improve trust is to use a voting-style algorithm for gradients. In this situation, small subsets of workers compute gradients <strong>on the same examples</strong> and are synchronized. The requester only “accepts” the gradient returned by the majority. If any one of them defects from the majority, the requester does not pay them. This is vulnerable if a majority of workers “collude” against the requester, but it is possibly cheaper and simpler than verifiable computation.</p>

<p>The second trust issue can be mitigated by having a tight feedback loop between the requester and the workers; i.e., they receive small micropayments for computing individual gradients rather than one bulk payment at the end. This allows the workers to stop computing gradients if they don’t get payment (a kind of tit-for-tat relationship).</p>

<h2 id="federated-learning">Federated learning</h2>
<p>Sometimes, the data that we train on is more valuable than the compute, and the data itself is already distributed across the network. In this setting, we might want a network of nodes (e.g., sensors, users, cameras, medical devices) to provide gradients for our model but not give away their data. This is the <a href="https://federated.withgoogle.com/">federated learning</a> setting.</p>

<p>The idea is similar: ask each worker to train a local model of their data, or even compute gradient estimates on their local copy of the data and send it to a central server. With some tricks (e.g., <a href="https://arxiv.org/pdf/1607.00133.pdf">adding Gaussian noise to the data</a>), the hope is that the requester can’t reconstruct the data from these messages. Because we can’t ask the workers to give up their data, we have to trust the workers that their gradients are correct. But this is another interesting setting where we can harness a large number of workers to train a model that would otherwise be infeasible.</p>

  </div>

  
    <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'http://localhost:4000/blog/2019/10/06/challenges-in-distributed-machine-learning.html';
      this.page.identifier = 'http://localhost:4000/blog/2019/10/06/challenges-in-distributed-machine-learning.html';
    };
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://christiancosgrove.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  
</article>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


      <footer class="site-footer">
        <!-- SVG icons from https://iconmonstr.com -->

        <!-- Github icon -->
        <span class="my-span-icon">
          <a href="https://github.com/christiancosgrove" aria-label="'s GitHub" title="'s GitHub">
            <svg class="my-svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
          </a>
        </span>

        <!-- Twitter icon -->
        <!-- <span class="my-span-icon">
          <a href="https://twitter.com/" aria-label="'s Twitter" title="'s Twitter">
            <svg class="my-svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z"/></svg>
          </a>
        </span> -->

        <!-- RSS icon -->
        
        <!-- Contact icon -->
        <!-- 
         -->

      </footer>
    </section>

    
      <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-99827000-1', 'auto');
        ga('send', 'pageview');
      </script>
    
  </body>
</html>
