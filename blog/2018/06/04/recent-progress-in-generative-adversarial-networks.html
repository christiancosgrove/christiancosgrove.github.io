<!DOCTYPE html>
<html lang="en">

  <head>

    <!-- Non social metatags -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="theme-color" content="#157878">

    

    <title>Recent Progress In Generative Adversarial Networks</title>

    
      <!-- Update your html tag to include the itemscope and itemtype attributes. -->
<html itemscope itemtype="http://schema.org/Article">












<!-- Place this data between the <head> tags of your website -->

  <meta name="author" content="Christian Cosgrove">

<meta name="description" content="" />





<!-- Schema.org markup for Google+ -->
<meta itemprop="name" content="Recent Progress In Generative Adversarial Networks">
<meta itemprop="description" content="">

  <meta itemprop="image" content="https://christiancosgrove.com/blog">


<!-- Twitter Card data -->
<meta name="twitter:card" content="summary_large_image">



<meta name="twitter:title" content="Recent Progress In Generative Adversarial Networks">
<meta name="twitter:description" content="">



<!-- Twitter summary card with large image must be at least 280x150px -->

  <meta name="twitter:image:src" content="https://christiancosgrove.com/blog">
  <meta property="twitter:image" content="https://christiancosgrove.com/blog">

<meta property="twitter:url" content="https://christiancosgrove.com/blog/2018/06/04/recent-progress-in-generative-adversarial-networks.html">

<!-- Open Graph data -->
<meta property="og:title" content="Recent Progress In Generative Adversarial Networks" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://christiancosgrove.com/blog/2018/06/04/recent-progress-in-generative-adversarial-networks.html" />

  <meta property="og:image" content="https://christiancosgrove.com/blog" />

<meta property="og:description" content="" />
<meta property="og:site_name" content="Ode to Gradients" />

  <meta property="article:published_time" content="2018-06-04T00:00:00-04:00" />














  





  




    

    <link rel="canonical" href="https://christiancosgrove.com/blog/2018/06/04/recent-progress-in-generative-adversarial-networks.html">

    

    <link rel="shortcut icon" href="https://christiancosgrove.com/favicon.ico">
    <meta name="robots" content="noarchive">

    <!-- <link rel="alternate" media="only screen and (max-width: 640px)" href="">
    <link rel="alternate" media="handheld" href=""> -->


    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/blog/assets/css/style.css?v=">
  </head>
  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    

    
      <a class="site-title" href="/blog/">Ode to Gradients</a>
    

    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="https://christiancosgrove.com/">About</a>
            <!-- 
            
            
              
              
            
          
            
            
          
            
            
           -->
        </div>
      </nav>
    
  </div>
</header>


    
    
    

    <section class="page-header">
      <h1 class="project-name">Recent Progress In Generative Adversarial Networks</h1>
      <h2 class="project-tagline"></h2>
      
      <!-- Post tagline -->
      
        <h2 class="project-date">
        <time datetime="2018-06-04T00:00:00-04:00" itemprop="datePublished">
          
          Jun 4, 2018
        </time>
        
        
          • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Christian Cosgrove</span></span>
        
        </h2>
      
      <!-- End: Post tagline -->
    </section>

    <section class="main-content">

      <article itemscope itemtype="http://schema.org/BlogPosting">

  <!-- <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Recent Progress In Generative Adversarial Networks</h1>
    <p class="post-meta">
      <time datetime="2018-06-04T00:00:00-04:00" itemprop="datePublished">
        
        Jun 4, 2018
      </time>
      </p>
  </header> -->
  <script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/jsxgraph/0.99.6/jsxgraphcore.js"></script>
  <div itemprop="articleBody">
    
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<p><a href="https://github.com/christiancosgrove/pytorch-sagan">Try out my SAGAN implementation</a></p>

<p><em>This article discusses self-attention generative adversarial networks, a new technique that improves the stability of generative adversarial networks and the quality of their samples. An orthogonal technique called spectral normalization is explained in <a href="/blog/2018/01/04/spectral-normalization-explained.html">this blog post</a></em>.</p>

<p>Since <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">generative adversarial networks</a> were conceived four years ago, startling improvements in the quality and diversity of samples have been achieved. Much of this progress has come from stabilizing the adversarial game between the discriminator and the generator, which can collapse if the discriminator can differeniate real and fake examples too easily. Some notable papers about GAN stabilization include</p>

<ul>
  <li><a href="https://arxiv.org/abs/1606.03498">Improved Techniques for Training GANs
</a>, which introduces new tricks for stabilizing GANs as well as a metric to evaluate the quality of GAN samples, the <em>Inception score</em>.</li>
  <li><a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a>, which puts forth theoretical justifications for using the a metric for which the generator always receives an appreciable gradient signal, regardless of how easily separable the fake and real distributions are.</li>
  <li><a href="https://arxiv.org/abs/1704.00028">Improved Training of Wasserstein GANs</a>, which enforces the <a href="/blog/2018/01/04/spectral-normalization-explained.html">K-Lipschitz regularization</a> on the discriminator required by the Wasserstein metric in a more graceful and stable way—the <em>gradient penalty</em>.</li>
  <li><a href="https://arxiv.org/abs/1802.05957">Spectral Normalization for Generative Adversarial Networks</a> imposes this K-Lipschitz regularization while maintaining the capacity of the discriminator as much as possible. Moreover, whereas gradient penalty can only be enforced at a finite number of points in the data space, <a href="/blog/2018/01/04/spectral-normalization-explained.html">spectral normalization ensures that the discriminator is K-Lipschitz <em>everywhere</em></a>.</li>
</ul>

<p>Many of the innovations above are agnostic to the problem where GANs are typically applied: <em>image generation</em>. In this setting, the generator is a deconvolutional neural network and the discriminator is a convolutional neural network. Typically, the discriminator is a series of local convolutional filters followed by nonlinearities, which successively downsample the image. The downsampling process terminates in a global average pooling, which compresses the activations to a single numeric output.</p>

<p>This average pooling treats distant points in the image as independent, which is certainly not the case in most natural images. For instance, suppose there is a real image in our training set</p>

<p><img src="https://christiancosgrove.com/blog/assets/images/cat.jpg" alt="real sample" /></p>

<p>and our generator produces an output</p>

<p><img src="https://christiancosgrove.com/blog/assets/images/cat-fake.jpg" alt="real sample" /></p>

<p>In this case, the generator matches the true data exactly except at a small spatial position where the generator mistakenly produces a red box. If the discriminator is using global average pooling, then the outputs of distant receptive fields—which cannot differentiate the two images—will be averaged with the outputs of few receptive fields that do contain the red box.</p>

<p><img src="https://christiancosgrove.com/blog/assets/images/cat-fake-receptive.jpg" alt="real sample" /></p>

<p>This averaging destroys the signal that the discriminator needs to differentiate the two images, making it difficult for the generator to improve further.</p>

<p>The independence assumption of global pooling (and direct linear layers as well) makes it difficult for the discriminator to detect errors in the global configuration of features—this is one reason why it has been difficult to generate high-quality ImageNet samples.</p>

<p><a href="https://arxiv.org/abs/1711.07971">Non-local Neural Networks</a> (Wang et al.) address the problem of CNNs processing information too locally by introducing a <em>self-attention</em> mechanism, where the output of each activation is modulated by a subset of other activations. This helps capture dependencies between distant parts of the images and allows the CNN to attend to smaller parts of the image if necessary.</p>

<p><a href="https://arxiv.org/abs/1805.08318">Self-Attention Generative Adversarial Networks</a> (SAGAN) apply this mechanism to the GAN discriminator, mitigating the problem above.</p>

<p>In a self-attention block, we temporarily ignore all spatial structure by flattening each channel into a single vector, after computing a local linear transformation on each pixel:</p>

<p>\begin{equation*}
	o_\text{self-attention} = \underbrace{\text{softmax}(\overbrace{x^T W_f^T}^{HW\times C}\,\,\overbrace{W_g x}^{C\times HW})}_{HW \times HW} \underbrace{W_h x}_{HW \times C}
\end{equation*}</p>

<p>where \(H\) is the height, \(W\) is the width, and \(C\) is the number of channels in the self-attention layer.</p>

<p>The softmax attention output weighs the influence of distant outputs on the output at each position in the discriminator. This allows the discriminator to explicitly compare outputs at separated points in the image and, in addition, focus all of its attention on a specific location in the image.</p>

<p>The self-attention output is added to the original output and multiplied by a weight parameter. At the start of training, this weight is zero; as the generated samples become more difficult to distinguish from the real samples, this weight can be increased so that the discriminator can use self-attention.</p>

<p><img src="https://christiancosgrove.com/blog/assets/images/sagan.png" alt="Self-attention" />
(<a href="https://arxiv.org/abs/1805.08318">taken from Zhang et al.</a>)</p>

<p>The above self-attention visualizations from the SAGAN paper demonstrate how the attention units capture the object shape. In most cases, the attention maps capture local feature similarities (2nd from left, top) as a convolutional network would. However, in some cases (1st and second from right, top and bottom), the self-attention layer detects detects correlations between features in distant regions of the image. This explains why the <em>global structure</em> of SAGAN samples, seen above, looks so convincing.
Below is an example of the self-attention maps from <a href="https://github.com/christiancosgrove/pytorch-sagan">my implementation of SAGAN</a> for the top-left-hand corner of the image.</p>

<p><img src="https://github.com/christiancosgrove/pytorch-sagan/raw/master/generation_results/attention_fashion.gif?raw=true" alt="self-attention visualization" /></p>

<p>Introducing the self-attention mechanism to GANs improves the quality of samples considerably: the authors of SAGAN report an Inception score of 52.52, an enormous improvement on the already impressive result of 36.8 achieved by spectral normalization a few months ago. This improvement in quality and diversity metrics is reflected directly in the quality of SAGAN ImageNet samples; many are indistinguishable from real images.</p>

<p>In my opinion, self-attention GANs reveal the potential to improve GANs by incorporating better priors into models (in this case, our prior is to explicitly build in modeling of non-local feature dependencies into some layers of the discriminator). This approach to improving GANs stands in contrast to more explored approach thus far: changing the GAN training objective or discriminator regularization. We have seen before with <a href="https://arxiv.org/abs/1612.03242">LapGAN</a> and <a href="https://arxiv.org/abs/1710.10196">Progressive Growing GAN</a> that carefully designed discriminator architectures (i.e., model priors) can lead to improved sample quality and diversity. With the breakthrough of self-attention GANs, we see this line of work confirmed yet again. It is only a matter of time that further improvements based on this strategy will come, and this already impressive state of the art will be beaten once again.</p>

  </div>

  
    <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://christiancosgrove.com/blog/2018/06/04/recent-progress-in-generative-adversarial-networks.html';
      this.page.identifier = 'https://christiancosgrove.com/blog/2018/06/04/recent-progress-in-generative-adversarial-networks.html';
    };
    (function() {
      var d = document, s = d.createElement('script');
      s.src = 'https://christiancosgrove.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  
</article>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


      <footer class="site-footer">
        <!-- SVG icons from https://iconmonstr.com -->

        <!-- Github icon -->
        <span class="my-span-icon">
          <a href="https://github.com/christiancosgrove" aria-label="'s GitHub" title="'s GitHub">
            <svg class="my-svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
          </a>
        </span>

        <!-- Twitter icon -->
        <!-- <span class="my-span-icon">
          <a href="https://twitter.com/" aria-label="'s Twitter" title="'s Twitter">
            <svg class="my-svg-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z"/></svg>
          </a>
        </span> -->

        <!-- RSS icon -->
        
        <!-- Contact icon -->
        <!-- 
         -->

      </footer>
    </section>

    
      <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-99827000-1', 'auto');
        ga('send', 'pageview');
      </script>
    
  </body>
</html>
